p8105_hw5_hc3212
================
Hening CUi
11/14/2021

## Problem 1

Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a “tidy” pipeline, making use of purrr::map,
purrr::map2, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

Create a plot that shows the estimates and CIs for each city – check out
geom_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

``` r
homo_df =
  read.csv("homicide-data.csv") 
```

The Washington Post collected data of criminal homicides over the past
decade in different cities. The data set has 52179 cases, and 12
variables which included the time, location and type of the killing,
whether an arrest was made and basic demographic information about each
victim.

``` r
homo_clean =
  homo_df %>% 
  mutate(city_state = str_c(city, ",", state)) %>% 
  select(-city, -state) %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>% 
  group_by(city_state, disposition) %>% 
  summarize (total = n()) 
```

    ## `summarise()` has grouped output by 'city_state'. You can override using the `.groups` argument.

``` r
homo_clean %>% 
  pivot_wider(
    names_from = disposition,
    values_from = total
  ) %>% 
  knitr::kable()
```

| city_state        | Closed without arrest | Open/No arrest |
|:------------------|----------------------:|---------------:|
| Albuquerque,NM    |                    52 |             94 |
| Atlanta,GA        |                    58 |            315 |
| Baltimore,MD      |                   152 |           1673 |
| Baton Rouge,LA    |                    16 |            180 |
| Birmingham,AL     |                    64 |            283 |
| Boston,MA         |                    NA |            310 |
| Buffalo,NY        |                     8 |            311 |
| Charlotte,NC      |                    44 |            162 |
| Chicago,IL        |                   387 |           3686 |
| Cincinnati,OH     |                    49 |            260 |
| Columbus,OH       |                    80 |            495 |
| Dallas,TX         |                    78 |            676 |
| Denver,CO         |                    46 |            123 |
| Detroit,MI        |                    16 |           1466 |
| Durham,NC         |                    11 |             90 |
| Fort Worth,TX     |                    35 |            220 |
| Fresno,CA         |                    23 |            146 |
| Houston,TX        |                   346 |           1147 |
| Indianapolis,IN   |                   102 |            492 |
| Jacksonville,FL   |                   141 |            456 |
| Kansas City,MO    |                    36 |            450 |
| Las Vegas,NV      |                   175 |            397 |
| Long Beach,CA     |                    27 |            129 |
| Los Angeles,CA    |                    NA |           1106 |
| Louisville,KY     |                    NA |            261 |
| Memphis,TN        |                    50 |            433 |
| Miami,FL          |                    63 |            387 |
| Milwaukee,wI      |                    37 |            366 |
| Minneapolis,MN    |                    31 |            156 |
| Nashville,TN      |                    57 |            221 |
| New Orleans,LA    |                    98 |            832 |
| New York,NY       |                    17 |            226 |
| Oakland,CA        |                    NA |            508 |
| Oklahoma City,OK  |                    11 |            315 |
| Omaha,NE          |                    10 |            159 |
| Philadelphia,PA   |                    92 |           1268 |
| Phoenix,AZ        |                    96 |            408 |
| Pittsburgh,PA     |                    NA |            337 |
| Richmond,VA       |                    20 |             93 |
| Sacramento,CA     |                    23 |            116 |
| San Antonio,TX    |                    87 |            270 |
| San Bernardino,CA |                    19 |            151 |
| San Diego,CA      |                    64 |            111 |
| San Francisco,CA  |                     1 |            335 |
| Savannah,GA       |                    12 |            103 |
| St. Louis,MO      |                    40 |            865 |
| Stockton,CA       |                    11 |            255 |
| Tampa,FL          |                     8 |             87 |
| Tulsa,OK          |                    55 |            138 |
| Washington,DC     |                    74 |            515 |

For the city of Baltimore, MD, use the prop.test function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the broom::tidy to this object and pull
the estimated proportion and confidence intervals from the resulting
tidy dataframe.

``` r
homo_clean %>% 
  filter(city_state == "Baltimore,MD") %>% 
  select(-city_state) 
```

    ## Adding missing grouping variables: `city_state`

    ## # A tibble: 2 × 3
    ## # Groups:   city_state [1]
    ##   city_state   disposition           total
    ##   <chr>        <chr>                 <int>
    ## 1 Baltimore,MD Closed without arrest   152
    ## 2 Baltimore,MD Open/No arrest         1673

## Problem 2

Creat a function to read data.

``` r
read = function(name) {
  path = str_c("./data/", name)
  obser = read.csv(path)
  return(obser)
}
```

Reading the data and tidy it.

``` r
study_df = tibble(
  filename = list.files("data" )
) %>% 
  mutate(outputlist = purrr::map(.x = as.character(filename), ~read(.x))) %>% 
  unnest(outputlist) %>% 
  mutate(subject_ID = substr(filename, 1, 6 ),
           arm = substr(filename, 1, 3)) %>% 
  select(-filename) %>% 
  relocate(subject_ID, arm) %>% 
  janitor::clean_names() %>% 
  drop_na()
```

Make the spaghetti plot.

``` r
study_df %>% 
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    values_to = "value"
  ) %>% 
  mutate(week = substr(week, 6, 6)) %>% 
  #group_by(subject_id) %>% 
  ggplot(aes(x = as.numeric(week), y = value, color = as.factor(subject_id))) +
  geom_line() + geom_point(aes(shape = as.factor(arm)), size = 2.5, alpha = 0.5) +
  labs(
    title = "The obervation of subject over time",
    y = "obervation value", 
    x = "Week",
    ) +
  scale_colour_hue("Subject_id")+
  scale_shape("Arm")
```

<img src="p8105_hw5_hc3212_files/figure-gfm/spaghetti-1.png" width="90%" />
It could find from the graph that most experiment group have higher
observation value than the control group.

## Problem 3

Load iris dataset.

``` r
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

Write the function of refill missing data

``` r
missing = function(a) {
  
  if(is.numeric(a)){
    mean_a = round(mean(a, na.rm = TRUE), digits = 2)
    a = replace_na(a, mean_a)
  }
  
  if (is.character(a)){
    a = replace_na(a, "virginica")
  }
  
  return(a)
}
```

Refill the data, and build new data set

``` r
iris_correct = map(iris_with_missing, missing)

iris_df = 
  tibble(
    Sepal.Length = iris_correct[["Sepal.Length"]],
    Sepal.Width = iris_correct[["Sepal.Width"]],
    Petal.Length = iris_correct[["Petal.Length"]],
    Petal.Width = iris_correct[["Petal.Width"]],
    Species = iris_correct[["Species"]]
  )

iris_df
```

    ## # A tibble: 150 × 5
    ##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    ##           <dbl>       <dbl>        <dbl>       <dbl> <chr>  
    ##  1         5.1          3.5         1.4         0.2  setosa 
    ##  2         4.9          3           1.4         0.2  setosa 
    ##  3         4.7          3.2         1.3         0.2  setosa 
    ##  4         4.6          3.1         1.5         1.19 setosa 
    ##  5         5            3.6         1.4         0.2  setosa 
    ##  6         5.4          3.9         1.7         0.4  setosa 
    ##  7         5.82         3.4         1.4         0.3  setosa 
    ##  8         5            3.4         1.5         0.2  setosa 
    ##  9         4.4          2.9         1.4         0.2  setosa 
    ## 10         4.9          3.1         3.77        0.1  setosa 
    ## # … with 140 more rows
